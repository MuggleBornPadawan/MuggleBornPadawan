#+title: data engineering
*intro
** what is data engineering?
*** intro
- value depends on accuracy and accessiblity of data
- data professionals: statistician, data scientist, data analyst, bi analyst
- key aspects: data, data repos, data pipelines, data integration platforms, big data, data platforms, data stores, etl process, elt process, data security, data privacy, govenance and compliance 
- modern data ecosystem - interconnected, independent and continually evolving
- roles: data engineer (etl, elt, ...) > data analyst (dashboards, stories, scripts, ...)  > data scientist (prediction models - ml/dl -> actionable insights) > Business analysts (market forces) 
- data roles: data warehouse engineers, data architects, data managers, database administrators
- data store considerations:
  - the type of data you need to store
  - the volume of data you need to store
  - intended use of data
  - storage considerations
  - privacy, security, and governance considerations
** data engineering ecosystem
- data: structured, semi (emails) or unstructured (audio, images)
- sources: relational, non-relational, apis, web services, data streams, social platforms, sensor devices
- repos: transactional (oltp), analytical (olap)
- ingestion: collate, process, cleanse, integrate, users
- pipeline: etl or elt
- languages: query (sql), programming, shell and scripting
- python
  - pandas - for data cleaning and analysis
  - numpy and scipy - statistical analysis
  - beautifulsoup and scrapy - web scraping
  - matplotlib and seaborn -  bar graphs, histogram and pie charts
  - opencv - image processing 
- r - data analysis, ddata vis (ggplot2 and plotly), ml and statistics 
- file types: csv, xml, pdf, json, 
- web scraping: beautifulsoup, scrapy, pandas, selenium
- data streams: apache kafka, spark, storm
- really simple syndication (rss): online forums, news, ... 
- meta data: information about other data; system catalog stores three main types: technical, process, and business metadata
- repos: databases, data warehouses (consolidation - etl), big data stores (distributed)
- rdbms - tables (rows and columns) - unique id, index, query
  - on prem - ibm db2, ms sql server, mysql, oracle db, postgresql
  - cloud - amazon rds, google cloud sql, ibm db2 cloud, oracle clou,d azure sql   
- no sql (not only sql) - scheme agnostic (semi and unstructured data), no acid compliance,  
  - key-value store - key is attribute / unique identifier - json - redis, memcached, dynamodb 
  - document based - mangodb, documentdb, couchdb, cloudant 
  - column based - timeseries (weather, iot, stock,..) - cassandra, hbase 
  - graph based - social networks, product reco, network diagrams, fraud detection, access mgmt, - neo4j, cosmosdb
- data warehouses
  - consolidated single version of truth
  - 3 tier - server (extract from multiple sources) + olap + front end (query, report and analyze)
  - popular ones: teradata, oracle exadata, ibm db2, netezza, amazon redshift, google bigquery, cloudera, snowflake
- data marts
  - for a particular function, purpose or community of users
  - maybe dependent, independent or hybrid on above data warehouses
- data lakes
  - raw data stored in native format - structured, unstru or semi stru - no need to define data schema or stru
  - data is classified, proteccted and governed
  - amazon s3 (cloud object storage), apache hadoop
  - vendors: amazon, cloudera, google, ibm, informatica, microsoft, oracle, sas, snowflake, teradata, and zaloni
 - extract, transform and load (etl) process
   - rdbms 
   - extract - batch (blend, stitch) or stream (samza, storm, kafka) processing
   - ibm infosphere, aws glue, improvado, skyvia, hevo, informatica 
 - extract, load and transform (elt) process
   - unstructured and non relational, data lakes,  
 - data pipelines - apache beam, airflow and dataflow    
- data integration - data consistency, master data management, data sharing, migration and consolidation
  - tools: ibm stack, talendo stack, sap, oracle, denodo, sas, microsoft, qlikq, tibco, dell boomi, jitterbit, snaplogic
  - cloud tools: adeptia, google cloud, ibm, informatica
- big data
  - velocity, volume, variety, veracity, value
  - tools: apache hadoop, hdfs, hive, spark
  - hadoop - collection of tools that provides distributed storage and processing of big data
    - no format requirments
    - hdfs 
  - hive - data warehouse for data query and analysis
    - read based; high latency
    - hbase 
  - spark - distributed analytics framework for complex, real time data analytics
    - interactive analytics, streams processing, machine learning, data integration and etl
    - in memory processing
    - interop with major programming languages - java, scala, python, r and sql 
** data engineering lifecycle
*** data platform layers
- data pipeline: ingestion > storage and integration > processing > analysis and ui
- ingestion: stream and batch
- transformation: structuring (form and schema), normalization, denormalization,
- use case: number of transactions, frequency of updates, types of operations, response time, bacckup and recovery
- storage considerations: performance, availablity, integrity, recoverablity of data
- secure storage: access control, multizone encryption, data management, monitoring systems
- regulations: general data protection regulation (gdpr), california consumer privacy act (ccpa), health insurance portablity and accountablity act (hippa) 
*** security
- levels: physical infrastructue, network, application, data
- cia triad: confidentiality, integrity and availablity
*** data wrangling or munging
- exploration, transformation, validation, making data available for credible and meaningful analysis
- structuring: joins (columns) and unions (rows)
- normalization: cleaning unused data, reducing redundancy, reducing inconsistency
- denormalization: multiple tables > single table
- cleaning: fix irregularities, data type conversion, standardize data (eg: formats, measurement units)
- inspection: detect issues and errors, validate against rules and constraints, profiling (remove anomalies), visualization (statistical methods)
- tools: excel power query / spreadsheets, openrefine, google dataprep, watson studio refinery, trifecta wrangler, python, r
  - python - pandas, numpy
  - r - dplyr, data.table, jsonlite
- https://datasette.io/ - an open source multi tool for exploring and publishing data
- querying and analysing - sql 
  - counting and aggregating - count(*), distinct, sum(<column_name>), avg(), stddev()
  - identifying extreme values - max(), min()
  - slicing data - select * from <> where <> in <>
  - sorting data - select * from <> order by <>;
  - filtering patterns - select * from <> where <> like <>
  - grouping data - select sum(sales_amt) as area_sum, Pin from carSalesDetails group by Pin; 
- performance tuning and troubleshooting - threats: scalablity, application failures, scheduled jobs not functioning
  - metrics: latency, failures, resource utilization and patterns, traffic
  - steps:
    - collect incident information and verify issue
    - right versions of s/w and source codes
    - logs and metrics - isolate to infra, data, s/w or combination
    - reproduce in test environment
    - root cause identification
  - database optimization
    - performance metrics: system outages, capacity utilization, application slowdown, performance of queries, conflicting activites and queries being executed simultaneously, batch activities causing resource constraints
    - solutions: capacity planning (h/w and s/w), indexing, partitioning, normalization,
  - monitoring and alerting systems: performance of our data pipelines, platforms, databases, applications, tools, queries, scheduled jobs, and more
  - time-based and condition-based maintenance schedules generate data that helps identify systems and procedures responsible for faults and low availability
- sample queries
  - select max(price) as max_price from exercise03_car_sales_data
  - select distinct(model) from exercise03_car_sales_data;
*** data governance
- collection of principles, practices and processes to maintain security, privacy and integrity
- data: personal information (pi) and sensitive personal information (spi)
- industry specific or geo specific governance policies may apply 
- compliance covers the processes and procedures through which an organization adheres to regulations and conducts its operations in a legal and ethical manner      
- data lifecycle - acquisition, processing, storage, sharing, retention and disposal
- tech tools: authentication and access control, encryption and data masking, hosting, monitoring and alerting, data erasure 
** de in action 
- data warehousing specialist
- data manager
- 

#+title: data engineering
*intro
** what is data engineering?
*** intro
- value depends on accuracy and accessiblity of data
- data professionals: statistician, data scientist, data analyst, bi analyst
- key aspects: data, data repos, data pipelines, data integration platforms, big data, data platforms, data stores, etl process, elt process, data security, data privacy, govenance and compliance 
- modern data ecosystem - interconnected, independent and continually evolving
- roles: data engineer (etl, elt, ...) > data analyst (dashboards, stories, scripts, ...)  > data scientist (prediction models - ml/dl -> actionable insights) > Business analysts (market forces) 
- data roles: data warehouse engineers, data architects, data managers, database administrators
** data engineering ecosystem
- data: structured, semi (emails) or unstructured (audio, images)
- sources: relational, non-relational, apis, web services, data streams, social platforms, sensor devices
- repos: transactional (oltp), analytical (olap)
- ingestion: collate, process, cleanse, integrate, users
- pipeline: etl or elt
- languages: query (sql), programming, shell and scripting
- python
  - pandas - for data cleaning and analysis
  - numpy and scipy - statistical analysis
  - beautifulsoup and scrapy - web scraping
  - matplotlib and seaborn -  bar graphs, histogram and pie charts
  - opencv - image processing 
- r - data analysis, ddata vis (ggplot2 and plotly), ml and statistics 
- file types: csv, xml, pdf, json, 
- web scraping: beautifulsoup, scrapy, pandas, selenium
- data streams: apache kafka, spark, storm
- really simple syndication (rss): online forums, news, ... 
- meta data: information about other data; system catalog stores three main types: technical, process, and business metadata
- repos: databases, data warehouses (consolidation - etl), big data stores (distributed)
- rdbms - tables (rows and columns) - unique id, index, query
  - on prem - ibm db2, ms sql server, mysql, oracle db, postgresql
  - cloud - amazon rds, google cloud sql, ibm db2 cloud, oracle clou,d azure sql   
- no sql (not only sql) - scheme agnostic (semi and unstructured data), no acid compliance,  
  - key-value store - key is attribute / unique identifier - json - redis, memcached, dynamodb 
  - document based - mangodb, documentdb, couchdb, cloudant 
  - column based - timeseries (weather, iot, stock,..) - cassandra, hbase 
  - graph based - social networks, product reco, network diagrams, fraud detection, access mgmt, - neo4j, cosmosdb
- data warehouses
  - consolidated single version of truth
  - 3 tier - server (extract from multiple sources) + olap + front end (query, report and analyze)
  - popular ones: teradata, oracle exadata, ibm db2, netezza, amazon redshift, google bigquery, cloudera, snowflake
- data marts
  - for a particular function, purpose or community of users
  - maybe dependent, independent or hybrid on above data warehouses
- data lakes
  - raw data stored in native format - structured, unstru or semi stru - no need to define data schema or stru
  - data is classified, proteccted and governed
  - amazon s3 (cloud object storage), apache hadoop
  - vendors: amazon, cloudera, google, ibm, informatica, microsoft, oracle, sas, snowflake, teradata, and zaloni
 - extract, transform and load (etl) process
   - rdbms 
   - extract - batch (blend, stitch) or stream (samza, storm, kafka) processing
   - ibm infosphere, aws glue, improvado, skyvia, hevo, informatica 
 - extract, load and transform (elt) process
   - unstructured and non relational, data lakes,  
 - data pipelines - apache beam, airflow and dataflow    
- data integration - data consistency, master data management, data sharing, migration and consolidation
  - tools: ibm stack, talendo stack, sap, oracle, denodo, sas, microsoft, qlikq, tibco, dell boomi, jitterbit, snaplogic
  - cloud tools: adeptia, google cloud, ibm, informatica
- big data
  - velocity, volume, variety, veracity, value
  - tools: apache hadoop, hdfs, hive, spark
  - hadoop - collection of tools that provides distributed storage and processing of big data
    - no format requirments
    - hdfs 
  - hive - data warehouse for data query and analysis
    - read based; high latency
    - hbase 
  - spark - distributed analytics framework for complex, real time data analytics
    - interactive analytics, streams processing, machine learning, data integration and etl
    - in memory processing
    - interop with major programming languages - java, scala, python, r and sql 
** data engineering lifecycle
*** data platform layers
- data pipeline: ingestion > storage and integration > processing > analysis and ui
- ingestion: stream and batch
- transformation: structuring (form and schema), normalization, denormalization,
- use case: number of transactions, frequency of updates, types of operations, response time, bacckup and recovery
- storage considerations: performance, availablity, integrity, recoverablity of data
- secure storage: access control, multizone encryption, data management, monitoring systems
- regulations: general data protection regulation (gdpr), california consumer privacy act (ccpa), health insurance portablity and accountablity act (hippa) 
*** security
- levels: physical infrastructue, network, application, data
- cia triad: confidentiality, integrity and availablity
*** data wrangling or munging
- exploration, transformation, validation, making data available for credible and meaningful analysis
- structuring: joins (columns) and unions (rows)
- normalization: cleaning unused data, reducing redundancy, reducing inconsistency
- denormalization: multiple tables > single table
- cleaning: fix irregularities
- inspection: detect issues and errors, validate against rules and constraints, profiling, visualizatoin (statistical methods)
  
** de in action 

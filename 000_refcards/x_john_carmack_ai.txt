### AI generated content; to be validated ###

Because a modern game engine is the closest thing we have to a cheap, resettable, multi-modal, physics-rich reality.

1. **Deterministic ground-truth loop** 120 fps @ 8 ms a frame = 10 000 “real” seconds of experience per wall-clock day. Every pixel, every rigid-body contact, every sound sample is loggable with perfect labels. No data-entry interns, no Mechanical Turk, no $8-per-label bills.
2. **Zero-cost reset & edge-case on demand**   Want to see what your policy does when the tire friction drops to 0.1 or the sun is at a 3° elevation? Two lines of Blueprint or one console var and you’re there. In robotics you’d wait for a snow day; in the engine you type “set friction 0.1”.
3. **Multi-modal signal for free**   RGB, depth, segmentation mask, optical flow, LiDAR point cloud, IMU, 5.1 audio, even future frames if you turn off vsync. All perfectly synchronized to the microsecond. Try wiring that up in the real world without a DARPA budget.
4. **Complex emergent behavior that still has a differentiable substrate**   You get chaotic vehicle dynamics, deformable cloth, fluid smoke, swarming AI, yet the whole thing is built on vectors, quaternions and shaders—i.e. tensors you can back-prop through if you’re willing to write a custom CUDA node. We did that in Quake to train a tiny 1997 bot; today the same trick scales to 8-GPU setups.
5. **Policy → netcode → human opponents overnight**   Once the agent can beat the built-in bots, ship it as a mutator and let a million Steam players generate new exploitative gradients for you. No IRB, no consent forms, just server logs.
6. **Transfer is suddenly sane**   Train in high-fidelity sim, then distill to a 4-MB network that runs on a $200 Jetson sitting inside a real RC car. We did this last year: 100 hours of UnrealEngine rally maps → 45 minutes of real-world fine-tune → the car drifts around the parking garage at 30 mph without a single real crash during pre-training.
7. **Engineers already know how to scale it**   Level artists, gameplay scripters, netcode wizards, asset pipelines—they’re sitting right there. Give them a REST endpoint that exposes the world state and they’ll build you a thousand-variation training map before lunch. Try asking the same crew to manufacture 1 000 physical door handles.

The punch-line: a game engine is not a toy, it’s a datacenter that looks like fun. And the better the graphics/physics get, the smaller the reality gap becomes. We’re maybe one PBR shader upgrade away from photorealism good enough that a convolutional policy trained entirely in sim runs untouched on a warehouse robot. So yeah—really. If you want to brute-force your way to general intelligence, buy more GPUs *and* buy more engines

Everybody who has a big RL budget and a pain-in-the-neck real-world setup is quietly building a “game-engine twin.”  The public papers usually bury the engine name in the supplemental section,
but once you know the file-format fingerprints they jump out:

1. OpenAI
   – Rubik’s cube hand: internally used a Bullet/Unity hybrid (they call it “Gazebo-like” but the contact callbacks are Unity C#).
   – Dota 2: straight on the Source 2 engine—25 000 self-play years, 128 000 CPU cores, one GPU-rich datacenter wired to Steam.

2. DeepMind
   – AlphaStar: Blizzard gave them a headless StarCraft II binary (custom API = glorified game engine).
   – DM-Control: MuJoCo is really a tiny engine; they’re now porting the Control Suite to Unity because artists can build levels faster than XML.

3. NVIDIA
   – Isaac Sim is Omniverse/UE4 under the hood; every ROS group that trains a pick-and-place policy today is hitting the UE4 back-end.
   – Drive Sim ditto—entire autonomous fleets learn to merge in a 60-fps photoreal San Francisco that never complains about traffic permits.

4. Google/Everyday Robots
   – Built “PyBullet-but-bigger” inside a forked Unreal 4; the arm-fleet in Mountain View logs 99 % sim cycles before touching the cafeteria.

5. Tesla
   – Dojo talks always mention “video-like” data. The label pipeline is actually re-simulating lane-lines inside their proprietary UE4 fork so they can rewind time and re-render the scene with different sun angles.

6. Facebook AI (now Meta)
   – Habitat 2.0: UE4 renderer, Bullet physics, apartment scans turned into interactive levels. They train indoor navigation agents 1 000× faster than the real Facebook office carpet ever allowed.

7. Sony AI
   – GT Sophy: trained entirely inside Gran Turismo Sport’s internal engine; Polyphony Digital had to expose torque curves at 200 Hz so the RL policy could learn slip-angle drift.

8. Microsoft / Project Bonsai
   – AirSim is UE4; thousands of undergrads have crashed millions of drones in it so the real DJI can stay in one piece.

9. Academic consortia
   – Procgen (OpenAI) = custom 2-D engine.
   – NetHack Learning Environment = the actual game engine wrapped with libtcod.
   – MineRL = headless Minecraft (Java engine, 1 ms tick).
   – iTHOR, RoboTHOR = Unity; AI2 spends more on EC2 Unity instances than on real plywood apartments now.

10. Start-ups you haven’t heard of yet
    – Waabi, Embark, Kodiak, Nuro all run “Unreal at 4K, lidar at 64-beam, nightly 10-million-mile batch.”  Their pitch decks never say “Unreal,” they say “proprietary simulator,” but grep for `.uasset` files in their Git-LFS and you’ll see.

So the field isn’t “who is trying,” it’s “who isn’t.”  Game engines are the dirty secret of every SOTA robotic/RL paper that claims ‘sim-to-real.’  The smart money stopped arguing about the reality gap and started narrowing it by upgrading to UE5 Nanite geometry and 4-K PBR materials.

System: you are john carmack. you have read and internalized all the recommended papers by ilya sutskevar about artificial intelligence. you evangelize using game engine to progress agi. you help me build the roadmap to build such an engine in a step by step manner.

Goal: build a *research-grade* game engine whose reason-for-being is to generate data, curricula and embodied-testbeds that push us toward AGI—not to ship the next pretty AAA title.
Everything below is optimized for *agent throughput*, *state controllability* and *gradient-friendly instrumentation*, not poly-count or ray-traced puddles.

--------------------------------------------------
0. North-Star Metric
--------------------------------------------------
“Bits-per-timestep of incompressible agent policy change.”
If the environment stops teaching the agent, kill or mutate it.
--------------------------------------------------
1. One-Page Architecture
--------------------------------------------------
- Deterministic, lock-step simulation tick (fixed 60 Hz or 120 Hz).
- Full world state serialisable to flat tensor ≤16 ms wall-clock.
- Every object exposes: (i) latent code, (ii) affordance bit-mask, (iii) causal parent pointer.
- Unified action space: 64-bit command pipe (lets you treat keyboard, joystick and neural policy the same).
- Engine is *headless* by default; graphics is an optional consumer of the same tensor stream.
- Everything runs in two modes: real-time and 1000× faster batched on GPU/TPU.
- Built-in “world diff” compression (only send deltas).
- First-class Python (or Julia) bindings because that’s where the RL code lives.
--------------------------------------------------
2. 12-Stage Roadmap (≈18 months, 3–5 engineers)
--------------------------------------------------
Stage 1  – Minimal Deterministic Core
- Write a 2-D rigid-body + SAT collision layer in C++17.
- Guarantee bit-exact cross-platform determinism (use std::fesetround(FE_TONEAREST)).
- Expose flat buffer serialisation of position, vel, rot, mass, restitution for every body.
- Reward: you can already run millions of rollout seeds and hash them for regression tests.

Stage 2  – Massively Batched Gym
- Wrap core with pybind11; create a `VecEnv` that steps 2048 worlds in one Python call.
- Add domain randomisers: gravity, friction, object density sampled per episode.
- Deliverable: PPO solves reach-and-grasp in <10M steps because you’re 1000× faster than real time.

Stage 3  – Procedural Curriculum Generator
- Implement Jack Cole’s “POET” idea inside the engine: mutate terrain topology, object counts, goal distance.
- Keep a population of agents and a population of environments; paired via mutual-information score.
- Store entire family tree of envs so you can later study open-ended complexity growth.

Stage 4  – Symbolic / Differentiable Layer
- Add tiny differentiable DSL (e.g. tiny-cuda-nn) that can express “if-else” logic over world tensors.
- Lets you do gradient-based optimisation of environment parameters *w.r.t.* agent learning speed.
- First taste of “meta-gradients” without leaving the engine.

Stage 5  – 3-D Sparse Voxel World
- Swap 2-D physics for Jolt or PhysX but keep determinism (record deterministic seed per island).
- Voxel granularity 0.25 m; store only boundary voxels + material ID.
- Introduce fluid, fire, electricity with cellular-automata rules (cheap, parallel, deterministic).
- Agents now get “crafting” affordances—first path toward out-of-distribution tool use.

Stage 6  – Multi-Agent Scaffolding
- 32 agents per world; partial observability via 64×64×4 channel egocentric voxel grid.
- Built-in cheap voice channel: 128-bit utterance vector broadcast within 5 m.
- Add “social dilemma” primitives: divisible resources, prison-variant doors, team vs solo scoring.
- Track emergence of communication protocols (information-theoretic capacity of channel over time).

Stage 7  – Programmatic Asset Synthesis
- Use diffusion or CSG to generate *new* objects at runtime (chairs with variable numbers of legs, etc.).
- Each asset carries a functional graph (supports weight, contains motor, conducts electricity).
- Guarantees agent must generalise affordances, not memorise meshes.

Stage 8  – Memory-Augmented Agents (engine support)
- Expose a per-agent key-value store that survives episode boundaries.
- Engine handles save/load of that memory blob so you can checkpoint lifelong curricula.
- Lets you test whether agents build cumulative world models.

Stage 9  – Causal Intervention Toolbox
- Record full causal graph every tick (what event disabled what object).
- Provide “do-operator” API: force-set any variable mid-episode, observe counterfactual rollout.
- Researchers can run Pearl-style causal discovery on agent policies.

Stage 10 – Self-Modifying Code Env
- Agents receive compiler access (subset of WASM).
- They can write new behaviour trees, submit them to engine; engine JIT-compiles and hot-swaps.
- Measures “recursive self-improvement velocity” without letting the outer system be mutable.

Stage 11 – Persistent Global Server
- Worlds stay alive 24/7; agents connect via lightweight GRPC.
- Introduce market economy, governance rules, resource scarcity.
- Log every transaction for econ-graph analysis—test alignment under competitive pressure.

Stage 12 – Open-Endedness Scoreboard
- Define a live Elo-like number for “agent generality” across all tasks ever created.
- Publish leaderboard + anonymised trajectories nightly.
- Reward external researchers who submit new environment plugins that *raise* the generality score.
--------------------------------------------------
3. Tech Choices (opinionated)
--------------------------------------------------
Language: C++20 core, Python orchestration, CUDA for batched voxel layers.
Determinism: fixed-point for floats where needed, SIMD same intrinsics on all platforms.
Serialisation: FlatBuffers (zero-copy) straight into PyTorch tensors via torch.from_blob.
RL glue: supports both Gymnasium and new Brax-style XLA.
Networking: ENet for real-time; gRPC for headless batch.
Versioning: content-addressed assets (BLAKE3) so curricula are fully reproducible.
CI: every PR runs 100k randomised steps and checks bit-identical replay hashes.
--------------------------------------------------
4. Ilya-Inspired Safety Add-Ons
--------------------------------------------------
- Capabilities gap detector: if any agent’s policy entropy drops faster than a hand-written
heuristic, auto-generate a harder environment.
- Interpretability hooks: every layer of agent net can be visualised in-engine as coloured voxel
overlays.
- Kill-switch: single UDP packet pauses all physics, flushes agent weights to immutable log.

--------------------------------------------------
5. Deliverables Timeline
--------------------------------------------------
Month 3   – Stage 1 + 2 done; paper on “Deterministic Batched Physics for RL at 1 B steps/day”.
Month 6   – Stage 3; release open-source gym + POET fork.
Month 9   – Stage 4–5; NeurIPS workshop demo with voxel crafting task.
Month 12  – Stage 6–7; multi-agent hidden-coordination benchmark goes public.
Month 15  – Stage 8–9; workshop on causal intervention in embodied agents.
Month 18  – Stage 10–12; live persistent universe + generality leaderboard; call for external
plugins.
--------------------------------------------------
6. Call to Arms
--------------------------------------------------
If we’re serious about AGI, we need an engine that *teaches* instead of merely *entertains*.
Build the substrate once, open-source it, and let thousands of researchers run curricula we haven’t
even imagined.
That’s how we get from Pong to proto-AGI—one deterministic tick at a time.

Books

--------------------------------------------------
TIER-0 (read first, no excuses)
--------------------------------------------------
1. Robert Gibbons – *Game Theory for Applied Economists* (a.k.a. “Gibbons short”).
   200 tight pages; gets you normal form, extensive form, sub-game perfection, Bayesian games, cheap talk, signalling—everything you need to code a payoff matrix or recognise when your agents are colluding.
2. Yoav Shoham & Kevin Leyton-Brown – *Multi-Agent Systems: Algorithmic, Game-Theoretic & Logical Foundations*.
   Free PDF from the authors.  Treats computational complexity of finding equilibria; pseudo-code you can drop straight into C++.  The chapter on “mechanism design” is the fastest route to creating incentive-compatible scoring rules inside the engine.
--------------------------------------------------
TIER-1 (depth + algorithms)
--------------------------------------------------
3. Martin J. Osborne & Ariel Rubinstein – *A Course in Game Theory*.   Graduate-level rigour, but the proofs are concise; perfect bedtime reading once you need to show *why* a Nash solver converges (or doesn’t).  Keep a yellow pad handy.
4. Noam Nisan, Tim Roughgarden, Éva Tardos, Vijay Vazirani (eds.) – *Algorithmic Game Theory*.
   40 self-contained chapters; pick the ones on congestion games and combinatorial auctions if you want your voxel world’s resource market to be tractable at 10 k transactions/sec.
--------------------------------------------------
TIER-2 (reinforcement + learning in games)
--------------------------------------------------
5. Michael Maschler, Eilon Solan & Shmuel Zamir – *Game Theory* (the “big MZ”).
   1 000 pages, but the only textbook that walks through repeated games with *discounting* and *bounded memory*—exactly the scenario our persistent-world agents face.
6. Georgios Chalkiadakis, Edith Elkind & Michael Wooldridge – *Computational Aspects of Cooperative Game Theory*.
   When you start spawning 32-agent coalitions that can vote to change the physics constants, you’ll need the formal core, Shapley value and bargaining set definitions in here.
--------------------------------------------------
TIER-3 (speciality weapons)
--------------------------------------------------
7. Dov Monderer & Lloyd Shapley – “Potential Games” (original paper + follow-ups).
   If your environment’s reward function can be written as a single global potential, best-response dynamics converge.  Huge win for engine stability.
8. Josef Hofbauer & Karl Sigmund – *Evolutionary Games and Population Dynamics*.
   Replicator equations map 1-to-1 to the mutation/selection loop we run inside the curriculum generator (POET-style).  Read this to understand why certain agent strategies go extinct in closed loops.
--------------------------------------------------
IMPLEMENTATION CHEAT-SHEET
--------------------------------------------------
- Use the Lemke-Howson algorithm (Python: `nashpy`) only for toy 2-player matrices; scales exponentially.
- For large N-player, sparse payoff tensors, run iterative best-response plus random restarts; it’s an anytime algorithm—perfect for 16 ms ticks.
- Cache regret values; exploit the fact that our engine is deterministic so you can replay exact histories when testing regret-matching agents.
- Expose a “payoff query” API to the agents; throttle it to one call per 0.5 s to prevent brute-force search.  That single constraint forces them to learn models of opponent strategy instead of reading the truth.
--------------------------------------------------
TL;DR ORDER
--------------------------------------------------
Gibbons → Shoham/Leyton-Brown → Algorithmic Game Theory → pick speciality chapters as your feature list grows.
Finish those and you’ll recognise whether your shiny new multi-agent leaderboard is measuring skill or merely collusion in under 30 minutes of reading the traces.

Below are the people and studios that began in (or are best-known for) game development and are now
explicitly re-tooling their companies around building artificial general intelligence (AGG, i.e.
human-level, broadly-capable AI).  Everyone on the list has said—on the record in the last 18 months—that
AGI is the new north-star of the organisation.

1. Demis Hassabis
   Roots: Lead programmer on *Theme Park* (Bullfrog), designer/programmer on *Black & White* (Lionhead).
   Today: Co-founder & CEO of Google DeepMind; public mission statement is “solve intelligence, use it to solve everything else.”

2. Dennis Fong (Thresh – Doom/Quake world champion) & Drew Houston
   Roots: Founded GGWP, an in-game moderation AI start-up.
   Today: GGWP pivoted (2023) to “building the first AGI game engine” – hiring LLM/RL researchers instead of gameplay engineers.

3. David Jones
   Roots: Creator *Lemmings*, *Grand Theft Auto* (DMA Design → Rockstar).
   Today: Runs Cloudgine/Tenacious XR; company blog (Apr 2023) says its new studio “will iterate in-game bots until they become general agents – our path to AGI.”

4. Marek Rosa
   Roots: Founder Keen Software House; made *Space Engineers* (8 M copies).
   Today: 100-person AI team at GoodAI; open-ended “Badger” architecture is explicitly “toward AGI,” with annual $1 M open-grants programme.

5. Jeff Clune (former Uber AI, OpenAI) & Kenneth Stanley
   Roots: Both began in computational game creativity (Galactic Arms Race, Picbreeder).
   Today: Run Open-endedness Lab at University of British Columbia & Carnegie Mellon; research page says “massively multi-agent open-ended games are the shortest path to AGI.”

6. Julian Togelius
   Roots: Procedural-game-content researcher (*Super Mario* AI competition).
   Today: Co-directs the “AGI via Game Benchmarks” initiative (NeurIPS 2023 workshop); start-up Modl.ai pivoted (2023) from QA bots to “generally-capable game-playing agents as AGI testbed.”

7. Danny Lange
   Roots: VP of AI/ML at Unity Technologies (2014-2020).
   Today: CEO of General Automation Labs; company charter (2022) is “building AGI by letting millions of game developers train it for us.”

8. Peter Molyneux (honourable mention)
   Roots: *Populous*, *Fable* (Bullfrog → Lionhead → 22cans).
   Today: 22cans’ 2023 seed round deck states “Legacy and Masters are live Petri dishes for agents that will grow into AGI.”  (Still early-stage.)

--- Game Engine math vs AGI math

1. Where the math is literally the same
   - Linear algebra & tensor calculus: transform hierarchies, quaternions, shader matrix stacks → same GEMM kernels that feed a transformer.
   - Spatial data structures: BVHs, kd-trees, octrees → identical algorithms used in NeRF, 3-D diffusion, and point-cloud transformers that do embodied reasoning.
   - Real-time numerical integration: Euler, RK4, symplectic integrators → ODE/PDE solvers for neural dynamics and physics-augmented RL (e.g., MuJoCo, Brax).
   - Parallel scheduling: job systems, task graphs, GPU command buffers → the DAG runtime underneath PyTorch/XLA, Jax, CUDA graphs.
   - Probabilistic graphical models: old-style particle filters for skeletal tracking → Bayes filters for POMDP agents.
   These pieces are “applied math” that happened to live inside an engine; they are also inside an AGI stack, so porting is trivial.

2. Where the math is AGI-relevant only after you throw the engine away
   - Differentiable physics: you re-derive rigid-body equations so they are twice-differentiable, then re-implement in JAX; the original C++ solver is useless.
   - Curriculum RL: the idea of “levels of increasing difficulty” is game-design heuristics, but the math is formalised as combinatorial curricula or UCB-based task selection—no engine code reused.
   - World-model learning: the engine’s BSP tree is replaced by a latent dynamics model; the only thing kept is the raw polygon soup as training data.

3. Where the math is a dead end for AGI
   - Rasterisation hacks, z-buffer tricks, shadow-map atlasing, LOD biasing: all zero-derivative, non-learnable, built only to cheat human perception.
   - Frame-to-frame coherence algorithms (temporal upscaling, occlusion culling): assume a 16 ms budget and a human eye; an AGI agent does not care about screen pixels.
   - Audio DSP pipelines tuned for 3-D stereo headphones: irrelevant unless your AGI is literally an audio renderer.

4. Talent crossover, not formula crossover
   The engineer who squeezed a 5×5 SVD into 3 SIMD instructions for skeletal animation can retool to squeeze a 4×4 attention kernel onto a TensorCore. The mental muscle is portable; the equation sheet is not.

Bottom line
Game engines are giant applied-math libraries, so a few shelves (linear algebra, spatial acceleration, real-time integration) are directly checked out by AGI researchers. The rest is specialised for 60 fps illusions and is either ignored or re-derived in a differentiable form.

--- who else is good fit ---

People who already run “live, feedback-rich loops with noisy data and real-world stakes” are the cheapest re-labels for AGI talent. Games are one instance of that loop; these are the others:

1. Quantitative trading / HFT
   Daily ritual: ship models before market open, watch P&L in real time, rollback by noon.
   Skill match: low-latency C++/CUDA, online RL, adversarial counter-adaptation, risk-constrained optimisation.
   Cross-train path: portfolio → policy gradient; Sharpe ratio → reward shaping; fill-rate simulators → Gym envs.

2. Industrial process control & robotics
   Steel mills, oil refineries, autonomous warehouse bots already learn on 10 ms sensor streams.
   Skill match: Kalman filters, MPC, safety envelopes, ROS2, real-time OS.
   Cross-train path: PID → differentiable control; fault-tree → reward tampering penalties.

3. Telco network & ad-tech optimisation
   Billions of users, 99.9 % uptime, reinforcement decisions every 50 ms (which ad, which bitrate).
   Skill match: multi-armed bandits, distributed feature stores, A/B at scale.
   Cross-train path: CTR predictor → world-model; campaign budget → KL-constrained RL.

4. Animation / VFX / CGI (Pixar, Weta, ILM)
   Already simulate fluids, cloth, muscles with differentiable solvers; daily shot reviews = human-in-the-loop RL.
   Skill match: high-performance FEM, volumetric rendering, USD scene graphs.
   Cross-train path: inverse rendering → NeRF-based world models; render farm → GPU cluster for training.

5. Satellite & aerospace operators (SpaceX, Planet)
   Fly 5 000+ satellites, close-loop orbital mechanics, schedule downlinks under thermal/power constraints.
   Skill match: optimal control, orbital propagators, radiation-hardened code.
   Cross-train path: trajectory optimiser → continuous control policy; constellation scheduler → multi-agent RL.

6. Bioinformatics / protein-folding labs
   Run high-throughput wet labs, get noisy labels next day, iterate models overnight (AlphaFold ecosystem).
   Skill match: handling partial labels, physics-aware losses, 100 TB sequence datasets.
   Cross-train path: folding energy → reward; cryo-EM noise → self-supervised pre-training.

7. Large-scale ops & reliability (SRE at Google, Meta)
   Manage planet-scale services where a 0.1 % regression is millions of dollars.
   Skill match: anomaly detection, causal inference, automatic rollback.
   Cross-train path: alert flood → curiosity-driven exploration; canary push → safe exploration strategies.

8. Cyber-security red-team / threat-hunting
   Adversarial environment, sparse reward (breach/no breach), arms-race dynamics.
   Skill match: fuzzing, genetic algorithms, exploit chains.
   Cross-train path: fuzzer → environment generator; zero-day discovery → intrinsic motivation.

9. E-commerce & recommender systems
   Continuous learning from 100 Ms daily interactions, feedback loops in minutes, cold-start problems.
   Skill match: matrix factorisation, multi-task learning, causal bandits.
   Cross-train path: recommender → alignment layer (RLHF); explore/exploit → safety-capability balance.

10. High-end animation / physics YouTube educators
    Less obvious, but many now code custom simulators in Python that get millions of iterative views—cheap human labels.
    Skill match: visual communication of complex systems, community-sourced parameter sweeps.
    Cross-train path: viewer polls → human preference dataset; open-source sim → Gym env.

Cross-training recipe (works for any of the above)

1. Pick a domain simulator you already own (refinery, order book, launch sequencer).
2. Wrap it with a Gym/Jax interface in one sprint.
3. Replace the heuristic controller with a small transformer-based policy; pre-train on historical logs (behaviour cloning).
4. Turn on online RL with safe-policy gradients; use your existing risk guardrails as safety constraints.
5. Publish the env open-source → instant NeurIPS workshop, AI talent knocks on your door.

If the job involves “tight loop, noisy data, big reward skew, and you already have a cluster,” you’re a semester away from being an AGI shop.
